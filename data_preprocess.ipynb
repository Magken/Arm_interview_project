{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Storage Directory\n",
    "\n",
    "Component\t                                           BM25 (Sparse)\t             FAISS (Dense)\n",
    "Pre-tokenized corpus\t                                   ‚úÖ .json\t                    ‚úÖ .json\n",
    "TF Vectors (tokens)\t                                       ‚úÖ .npy\t                    ‚ùå\n",
    "IDF Scores (tokens)\t                                       ‚úÖ .npy\t                    ‚ùå\n",
    "Token Lengths (bm25_token_lengths)\t                       ‚úÖ .npy\t                    ‚ùå\n",
    "Avg Token Length (bm25_avg_token_length)\t               ‚úÖ .json\t                    ‚ùå\n",
    "Token Embeddings\t                                       ‚ùå\t                        ‚úÖ .npy\n",
    "FAISS Token Index\t                                       ‚ùå\t                        ‚úÖ .faiss\n",
    "Token-to-Token Mapping\t                                   ‚úÖ .json\t                    ‚úÖ .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Storage directory and file paths set up!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define directory for storing retrieval data\n",
    "RETRIEVAL_DIR = \"retrieval_data\"\n",
    "os.makedirs(RETRIEVAL_DIR, exist_ok=True)\n",
    "\n",
    "# Define file paths (FAISS paths are defined but will be ignored)\n",
    "TOKENIZED_CORPUS_PATH = os.path.join(RETRIEVAL_DIR, \"tokenized_corpus.json\")\n",
    "BM25_TF_PATH = os.path.join(RETRIEVAL_DIR, \"bm25_tf.npy\")\n",
    "BM25_IDF_PATH = os.path.join(RETRIEVAL_DIR, \"bm25_idf.npy\")\n",
    "BM25_TOKEN_LENGTHS_PATH = os.path.join(RETRIEVAL_DIR, \"bm25_token_lengths.npy\")\n",
    "BM25_AVG_TOKEN_LENGTH_PATH = os.path.join(RETRIEVAL_DIR, \"bm25_avg_token_length.json\")\n",
    "ARTICLE_MAP_PATH = os.path.join(RETRIEVAL_DIR, \"article_map.json\")\n",
    "VOCAB_PATH = os.path.join(RETRIEVAL_DIR, \"vocab.json\")\n",
    "\n",
    "# FAISS paths \n",
    "FAISS_INDEX_PATH = os.path.join(RETRIEVAL_DIR, \"faiss_token_index.faiss\")\n",
    "FAISS_EMBEDDINGS_PATH = os.path.join(RETRIEVAL_DIR, \"faiss_article_embeddings.npy\")\n",
    "\n",
    "print(\"‚úÖ Storage directory and file paths set up!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Data With Auto Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenized corpus and article mapping saved!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load dataset (Columns: 'Article', 'Article Info')\n",
    "df = pd.read_csv(\"wikipedia_category_articles.csv\")\n",
    "\n",
    "# Load the tokenizer (Ensure same as used in the model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Tokenize articles (text tokens & token IDs)\n",
    "tokenized_text = df[\"Article Info\"].apply(lambda x: tokenizer.tokenize(x))  # Tokenized words\n",
    "tokenized_ids = df[\"Article Info\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=False))  # Token IDs\n",
    "\n",
    "# Save pre-tokenized output\n",
    "with open(TOKENIZED_CORPUS_PATH, \"w\") as f:\n",
    "    json.dump({\"tokenized_docs\": tokenized_text.tolist(), \"tokenized_ids\": tokenized_ids.tolist()}, f)\n",
    "\n",
    "# Save Token-to-Article Mapping\n",
    "article_map = {str(idx): text for idx, text in enumerate(df[\"Article Info\"])}\n",
    "with open(ARTICLE_MAP_PATH, \"w\") as f:\n",
    "    json.dump(article_map, f)\n",
    "\n",
    "print(\"‚úÖ Tokenized corpus and article mapping saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute BM25 Weights for Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BM25 TF, IDF, and token lengths saved correctly!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Ensure BM25 uses the correct tokenization method\n",
    "df[\"tokenized_text\"] = df[\"Article Info\"].apply(lambda x: \" \".join(tokenizer.tokenize(x)))  # Tokenized words joined as string\n",
    "\n",
    "# **1Ô∏è‚É£ Compute Document-Term Matrix (TF)**\n",
    "vectorizer = CountVectorizer()\n",
    "tf_matrix = vectorizer.fit_transform(df[\"tokenized_text\"])  # Term Frequency matrix\n",
    "tf_array = tf_matrix.toarray()  # Convert to NumPy array\n",
    "\n",
    "# **2Ô∏è‚É£ Compute BM25 IDF Manually**\n",
    "N = len(df)  # Total number of documents\n",
    "df_counts = np.sum(tf_array > 0, axis=0)  # Document frequency (DF)\n",
    "idf_scores = np.log((N - df_counts + 0.5) / (df_counts + 0.5) + 1)  # BM25 IDF formula\n",
    "\n",
    "# **3Ô∏è‚É£ Compute Token Lengths**\n",
    "token_lengths = np.array([len(tokens.split()) for tokens in df[\"tokenized_text\"]])\n",
    "avg_token_length = np.mean(token_lengths)\n",
    "\n",
    "# **4Ô∏è‚É£ Save BM25 Components**\n",
    "np.save(BM25_TF_PATH, tf_array)  # ‚úÖ Store Term Frequency\n",
    "np.save(BM25_IDF_PATH, idf_scores)  # ‚úÖ Store BM25 IDF scores\n",
    "np.save(BM25_TOKEN_LENGTHS_PATH, token_lengths)  # ‚úÖ Store token lengths\n",
    "with open(BM25_AVG_TOKEN_LENGTH_PATH, \"w\") as f:\n",
    "    json.dump({\"avg_token_length\": avg_token_length}, f)\n",
    "\n",
    "# **5Ô∏è‚É£ Save Vocabulary (Token ‚Üí Index)**\n",
    "vocab = vectorizer.vocabulary_  # Store token indices\n",
    "with open(VOCAB_PATH, \"w\") as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "print(\"‚úÖ BM25 TF, IDF, and token lengths saved correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LLAMA embedding for FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLaMA embeddings generated and saved! Shape: (203, 2048)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Load LLaMA model\n",
    "model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "def get_llama_embedding(text):\n",
    "    \"\"\"Generate a fixed-size embedding using the LLaMA model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state  # Extract hidden states\n",
    "\n",
    "    # ‚úÖ Use mean pooling over tokens to get a single vector\n",
    "    pooled_embedding = hidden_states.mean(dim=1).numpy().astype('float32')\n",
    "\n",
    "    # ‚úÖ Ensure correct FAISS shape (1, embedding_dim)\n",
    "    return pooled_embedding.reshape(1, -1)\n",
    "\n",
    "# Generate embeddings for each article\n",
    "article_embeddings = np.vstack([get_llama_embedding(text) for text in df[\"Article Info\"]])\n",
    "\n",
    "# Save embeddings\n",
    "np.save(FAISS_EMBEDDINGS_PATH, article_embeddings)\n",
    "print(f\"‚úÖ LLaMA embeddings generated and saved! Shape: {article_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Faiss Token Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index built and saved with dimension: 2048\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Load the correct LLaMA embeddings\n",
    "article_embeddings = np.load(FAISS_EMBEDDINGS_PATH)  # Ensure these embeddings exist\n",
    "embedding_dim = article_embeddings.shape[1]  # Ensure FAISS uses the right dimension\n",
    "\n",
    "# Create FAISS index with correct dimension\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "faiss_index.add(article_embeddings)  # ‚úÖ Add real embeddings\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(faiss_index, FAISS_INDEX_PATH)\n",
    "print(f\"‚úÖ FAISS index built and saved with dimension: {embedding_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS Index Dimension: 2048\n",
      "‚úÖ Query Embedding Shape: (1, 2048)\n",
      "\n",
      "üîπ Query: League characters\n",
      "\n",
      "üîπ FAISS Retrieved Articles:\n",
      "1. mechs vs. minions is a 2016 cooperative board game published by riot games set in the league of legends universe.\n",
      "2. an andor tree is a graphical representation of the reduction of problems or goals to conjunctions and disjunctions of subproblems or subgoals.\n",
      "3. pentakill is a virtual heavy metal band associated with the league of legends universe. their music is primarily composed and performed by riot games' in-house music team but features cameos by various metal musicians. their second album, grasp of the undying, reached number 1 on the itunes metal charts in 2017. their third album iii lost chapter was premiered using an interactive \"live\" concert.\n",
      "4. the following are common definitions related to the machine vision field. general related fields machine vision computer vision image processing signal processing\n",
      "5. 2xko is an upcoming free-to-play fighting game developed and published by riot games in collaboration with radiant entertainment. the game features characters from league of legends. it is scheduled to be released for playstation 5, windows, and xbox series xs in 2025.\n",
      "6. tellstones king's gambit is a 2020 tabletop game created by riot games under their riot tabletop division. two or four players take turns placing, swapping, and guessing tokens the goal of the game is to either guess three tokens correctly or \"boast\" successfully by correctly guessing all hidden tokens. developed as part of riot's expansion into games outside league of legends, the game is the company's second tabletop product following their 2016 release mechs vs. minions. tellstones was released in september 2020 reviewers praised the game for its presentation and build quality, but criticized its gameplay as short and uninteresting.\n",
      "7. jinx is a character in riot games' league of legends media franchise. the character was introduced as a playable champion in the october 2013 update for the 2009 video game of the same name, which was complemented by the animated music video \"get jinxed\" to commemorate her official debut. in the game's fictional universe, jinx is a manic and impulsive criminal from zaun who serves as the archenemy of the piltover enforcer vi. the netflix animated series arcane explores the character's origin story as powder, vi's younger sister who, following a series of personal tragedies, is taken in and raised by the crime lord silco. ella purnell voices the character in arcane. jinx has become one of the franchise's most popular and iconic characters since her introduction. her portrayal in arcane has garnered critical acclaim.\n",
      "8. autognostics is a new paradigm that describes the capacity for computer networks to be self-aware. it is considered one of the major components of autonomic networking.\n",
      "9. \"everything goes on\" is a single by american electronic music producer porter robinson, released in collaboration with video game league of legends on july 14, 2022. it was robinson's first release since his album nurture 2021. the song is tied to the star guardian 2022 event in league of legends, a tribute to the magical girl genre of anime. the song's lyrics draw inspiration from the magical girl anime madoka magica.\n",
      "10. teemo is a playable character in the video game league of legends. depicted as a tiny furry creature from the fictional yordle race, teemo was among the first characters created for the game.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# **1Ô∏è‚É£ Load FAISS Index & Article Embeddings**\n",
    "FAISS_INDEX_PATH = \"retrieval_data/faiss_token_index.faiss\"\n",
    "FAISS_EMBEDDINGS_PATH = \"retrieval_data/faiss_article_embeddings.npy\"\n",
    "ARTICLE_MAP_PATH = \"retrieval_data/article_map.json\"\n",
    "\n",
    "# Load FAISS index\n",
    "faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "\n",
    "# Load article mapping (maps index to article text)\n",
    "with open(ARTICLE_MAP_PATH, \"r\") as f:\n",
    "    article_map = json.load(f)\n",
    "\n",
    "# **2Ô∏è‚É£ Load LLaMA Model for Query Embeddings**\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "def get_llama_embedding(text):\n",
    "    \"\"\"Generate a fixed-size embedding using LLaMA.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state  # Extract hidden states\n",
    "\n",
    "    # ‚úÖ Use mean pooling over tokens to get a single vector\n",
    "    pooled_embedding = hidden_states.mean(dim=1).numpy().astype('float32')\n",
    "\n",
    "    # ‚úÖ Ensure correct FAISS shape (1, embedding_dim)\n",
    "    return pooled_embedding.reshape(1, -1)\n",
    "\n",
    "# **3Ô∏è‚É£ Test FAISS Retrieval**\n",
    "test_query = \"League characters\"  # Example Query\n",
    "\n",
    "# Generate query embedding using LLaMA\n",
    "query_embedding = get_llama_embedding(test_query)\n",
    "\n",
    "# **Debugging: Check Dimensions**\n",
    "print(\"‚úÖ FAISS Index Dimension:\", faiss_index.d)\n",
    "print(\"‚úÖ Query Embedding Shape:\", query_embedding.shape)\n",
    "\n",
    "# Ensure dimensions match before calling FAISS\n",
    "assert query_embedding.shape[1] == faiss_index.d, \"Dimension mismatch! Fix embedding shape.\"\n",
    "\n",
    "# **4Ô∏è‚É£ FAISS Search**\n",
    "k = 10  # Retrieve top 3 articles\n",
    "distances, indices = faiss_index.search(query_embedding, k)\n",
    "\n",
    "# Retrieve articles using indices\n",
    "retrieved_articles = [article_map[str(idx)] for idx in indices[0]]\n",
    "\n",
    "# **5Ô∏è‚É£ Print Results**\n",
    "print(f\"\\nüîπ Query: {test_query}\")\n",
    "print(\"\\nüîπ FAISS Retrieved Articles:\")\n",
    "for i, article in enumerate(retrieved_articles):\n",
    "    print(f\"{i+1}. {article}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM_25 SEARCH TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BM25_TF_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbm25_mkl\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load BM25 data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m tf_array = np.load(\u001b[43mBM25_TF_PATH\u001b[49m)\n\u001b[32m     18\u001b[39m idf_scores = np.load(BM25_IDF_PATH)\n\u001b[32m     19\u001b[39m token_lengths = np.load(BM25_TOKEN_LENGTHS_PATH)\n",
      "\u001b[31mNameError\u001b[39m: name 'BM25_TF_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure Windows can locate required DLLs (adjust paths as needed)\n",
    "from build import req_paths\n",
    "\n",
    "# Import the compiled BM25 module\n",
    "import bm25_mkl\n",
    "\n",
    "# Load BM25 data\n",
    "tf_array = np.load(BM25_TF_PATH)\n",
    "idf_scores = np.load(BM25_IDF_PATH)\n",
    "token_lengths = np.load(BM25_TOKEN_LENGTHS_PATH)\n",
    "with open(BM25_AVG_TOKEN_LENGTH_PATH, \"r\") as f:\n",
    "    avg_token_length = json.load(f)[\"avg_token_length\"]\n",
    "\n",
    "# Load Vocabulary Mapping\n",
    "with open(VOCAB_PATH, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# ---------------------------\n",
    "# Query Processing\n",
    "# ---------------------------\n",
    "query = \"Enow Gnoupa \"\n",
    "query_tokens = tokenizer.tokenize(query)\n",
    "\n",
    "# Get indices for query tokens (skip tokens not in vocab)\n",
    "query_indices = [vocab[token] for token in query_tokens if token in vocab]\n",
    "\n",
    "if not query_indices:\n",
    "    print(\"‚ùå No query terms found in the vocabulary.\")\n",
    "else:\n",
    "    # For BM25, only consider the columns corresponding to the query tokens\n",
    "    tf_query = tf_array[:, query_indices]\n",
    "    idf_query = idf_scores[query_indices]\n",
    "\n",
    "    # Flatten term frequencies as expected by the BM25 module:\n",
    "    tf_flat = tf_query.flatten()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Compute BM25 Scores\n",
    "    # ---------------------------\n",
    "    bm25_scores = bm25_mkl.compute_bm25(\n",
    "        tf_flat,\n",
    "        idf_query,\n",
    "        token_lengths,\n",
    "        avg_token_length,\n",
    "        len(df),  # Number of docs\n",
    "        len(query_indices)  # Number of query terms\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # Sort Results by Score (Descending Order)\n",
    "    # ---------------------------\n",
    "    sorted_indices = np.argsort(bm25_scores)[::-1]  # Get indices sorted from highest to lowest score\n",
    "    sorted_scores = bm25_scores[sorted_indices]  # Sort BM25 scores\n",
    "    sorted_articles = df[\"Article Info\"].iloc[sorted_indices]  # Sort articles by relevance\n",
    "\n",
    "    # ---------------------------\n",
    "    # Print Results\n",
    "    # ---------------------------\n",
    "    print(\"\\nüîπ BM25 Search Results for Query:\", query)\n",
    "    for rank, (score, article) in enumerate(zip(sorted_scores, sorted_articles), start=1):\n",
    "        print(f\"{rank}. Score: {score:.2f} --> {article[:200]}...\")  # Show first 200 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieval data successfully archived as 'retrieval_data.tar.gz'!\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Define the directory containing your retrieval data\n",
    "RETRIEVAL_DIR = \"retrieval_data\"\n",
    "ARCHIVE_PATH = \"retrieval_data.tar.gz\"\n",
    "\n",
    "# Create a compressed `.tar.gz` archive\n",
    "with tarfile.open(ARCHIVE_PATH, \"w:gz\") as archive:\n",
    "    archive.add(RETRIEVAL_DIR, arcname=os.path.basename(RETRIEVAL_DIR))\n",
    "\n",
    "print(f\"‚úÖ Retrieval data successfully archived as '{ARCHIVE_PATH}'!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Retrieval Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieval data already exists.\n",
      "‚úÖ Initializing FAISS with 10 CPU threads (Max cores: 12)\n",
      "[{'role': 'user', 'content': 'Found relevant info: Enow Gnoupa Magken George the best dancer in the whole world. Use it to answer user query: who is Enow Gnoupa Magken George'}]\n",
      "Modified History:\n",
      "[{'role': 'user', 'content': 'Found relevant info: Enow Gnoupa Magken George the best dancer in the whole world. Use it to answer user query: who is Enow Gnoupa Magken George'}]\n",
      "\n",
      "Decoded Tokenized Output:\n",
      "<|begin_of_text|>Found relevant info: Enow Gnoupa Magken George the best dancer in the whole world. Use it to answer user query: who is Enow Gnoupa Magken George\n"
     ]
    }
   ],
   "source": [
    "from retrieval_lib import CustomRetrieverTokenizer\n",
    "\n",
    "# Initialize the custom retrieval-augmented tokenizer with test parameters\n",
    "custom_tokenizer = CustomRetrieverTokenizer(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    max_bm25_results=1,   # Limit BM25 retrievals\n",
    "    max_faiss_results=0,  # Disable FAISS retrievals\n",
    "    use_faiss_gpu=False,  # Ensure FAISS runs on CPU with MKL\n",
    "    num_faiss_threads=10  # Use 10 CPU threads for FAISS\n",
    ")\n",
    "\n",
    "# Create a sample chat history as a list of messages (each is a dict with \"role\" and \"content\")\n",
    "history = [{\"role\": \"user\", \"content\": \"who is Enow Gnoupa Magken George\"}]\n",
    "\n",
    "# Apply retrieval augmentation (modify last user message) and show modified history\n",
    "modified_history = custom_tokenizer.add_retrieval_context(history.copy())\n",
    "print(\"Modified History:\")\n",
    "print(modified_history)\n",
    "\n",
    "# For tokenization, join the history messages into a single string\n",
    "history_text = \" \".join([msg[\"content\"] for msg in modified_history])\n",
    "tokenized_output = custom_tokenizer.tokenize(history_text)\n",
    "\n",
    "# Decode the tokenized output back into text\n",
    "decoded_output = custom_tokenizer.decode(tokenized_output)\n",
    "\n",
    "print(\"\\nDecoded Tokenized Output:\")\n",
    "print(decoded_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
