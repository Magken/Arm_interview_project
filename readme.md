## Project Overview

This project introduces a custom Tokenizer that extends the Hugging Face tokenizer framework, 
integrating advanced retrieval capabilities with inbuilt Retrieval-Augmented Generation (RAG) support. 
It combines FAISS-based vector search with a custom BM25 ranking algorithm accelerated using Intel's Math Kernel Library (MKL), 
allowing for high-performance hybrid retrieval. The tokenizer is designed to work seamlessly within modern NLP pipelines, 
offering both dense and sparse retrieval modes, making it ideal for knowledge-enhanced tasks like document search, 
context injection for LLMs, and domain-specific chatbots.

The project was tested using CPU-based inferencing with the meta-llama/Llama-3.2-1B-Instruct model. 
The pipeline operates as follows: user input is first tokenized using the custom tokenizer, 
during which FAISS and the MKL-accelerated BM25 retrieval are executed directly at the tokenization level. 
The resulting contextually enriched input is then passed to the model for generation. 
This design enables fast, on-the-fly retrieval without requiring external pre-processing, 
making the system lightweight and easily deployable even in CPU-constrained environments (This specific model Was deployed with CPU inferencing).


# Advantages

Tight Integration: Retrieval (FAISS and BM25) happens within the tokenization step, streamlining the workflow and reducing latency by eliminating separate preprocessing pipelines.

Hybrid Retrieval: Supports both dense (FAISS) and sparse (BM25) retrieval, allowing flexible use cases and improving recall across diverse query types.

CPU-Friendly Deployment: Designed with CPU-based inferencing in mind, making it ideal for low-resource or edge environments.

Modular and Extensible: Built as a subclass of the Hugging Face tokenizer, it fits naturally into transformer workflows and can be easily extended or integrated with existing models.

MKL Acceleration: The use of Intel MKL significantly boosts BM25 performance, making sparse retrieval practical even on large corpora.

# Disadvantages

Coupled Architecture: By embedding retrieval logic in the tokenizer, the design becomes less modular, making it harder to swap out components independently (e.g., using a different retriever).

Tokenization Overhead: Retrieval adds computation time during tokenization, which could slow down batch processing or streaming applications.

Complex Debugging: Errors in the retrieval layer may appear during tokenization, making it harder to isolate and troubleshoot issues.

Limited GPU Optimization: While FAISS supports GPU acceleration, the current design focuses on CPU usage â€” potentially leaving out performance gains for GPU-rich setups.

Model-Agnostic Challenges: Passing retrieved content directly into the model assumes a prompt structure that may not be optimal for all LLM architectures.


## File Structure



ğŸ“ File Structure
+ README.md â€” Project documentation (you're editing this!)

+ requirements.txt â€” Frozen Python dependencies

+ retrieval_data.tar.gz â€” Compressed snapshot of retrieval dataset

+ wikipedia_category_articles.csv â€” Source dataset used for indexing (from dataset_aquisition.  ipynb)

ğŸ“‚ BM25_cpp â€” C++ source code for BM25 with MKL acceleration
+ Function_Profiling/ â€” Performance profiling scripts (tested using DrMemory VSCode Profiler)

+ bm25_bindings.cpp â€” Pybind11 wrapper code

+ bm25_mkl.cpp â€” Core MKL-based BM25 implementation

+ bm25_mkl.h â€” C++ header for BM25 functions

ğŸ“‚ build â€” Compiled artifacts and built libraries (Important)
+ bm25_bindings.* â€” Build output (.exp, .lib, .obj)

+ bm25_mkl.* â€” Build output (.obj, .pyd for Python module)

+ build_test.ipynb â€” Test notebook for verifying compiled module

+ *.dll â€” Required runtime DLLs (e.g., Intel MKL, Python)

+ req_paths.py â€” Helper to set DLL & module paths (Important)

ğŸ“‚ retrieval_data â€” Serialized precomputed data (decompressed from retrieval_data.tar.gz)
+ article_map.json â€” Maps article IDs to text or categories

+ bm25_*.npy / *.json â€” Token frequencies, lengths, IDF values for BM25

+ faiss_*.npy / .faiss â€” Dense embeddings and FAISS index

+ tokenized_corpus.json â€” Tokenized version of the full corpus

ğŸ“‚ retrieval_lib â€” Python library code
+ __init__.py â€” Library init file

+ retrieval.py â€” Retrieval logic (BM25, FAISS integration)

+ tokenizer.py â€” Custom tokenizer subclassing HuggingFace


ğŸ§ª Jupyter Notebooks
+ data_preprocess.ipynb â€” Preprocessing pipeline (creating retrieval data)

+ dataset_aquisition.ipynb â€” Script for downloading/curating data (getting test data)

+ llama.ipynb â€” Inference notebook using Llama 3.2 1B CPU (main)


## (ignore this ore open readme file to view non sauished structure)

RAG_TOKENIZER/
â”‚
â”œâ”€â”€ README.md                         # Project documentation (you're editing this!)
â”œâ”€â”€ requirements.txt                  # Frozen Python dependencies
â”œâ”€â”€ retrieval_data.tar.gz             # Compressed snapshot of retrieval dataset
â”œâ”€â”€ wikipedia_category_articles.csv   # Source dataset used for indexing (from dataset_aquisition.ipynb)
â”‚
â”œâ”€â”€ BM25_cpp/                         # C++ source code for BM25 with MKL acceleration
â”‚   â”œâ”€â”€ Function_Profiling/           # Performance profiling scripts (tested using DrMemory VSCode Profiler)
â”‚   â”œâ”€â”€ bm25_bindings.cpp             # Pybind11 wrapper code
â”‚   â”œâ”€â”€ bm25_mkl.cpp                  # Core MKL-based BM25 implementation
â”‚   â””â”€â”€ bm25_mkl.h                    # C++ header for BM25 functions
â”‚
â”œâ”€â”€ build/                            # Compiled artifacts and built libraries (*Important*)
â”‚   â”œâ”€â”€ bm25_bindings.*               # Build output (.exp, .lib, .obj)
â”‚   â”œâ”€â”€ bm25_mkl.*                    # Build output (.obj, .pyd for Python module)
â”‚   â”œâ”€â”€ build_test.ipynb              # Test notebook for verifying compiled module
â”‚   â”œâ”€â”€ *.dll                         # Required runtime DLLs (e.g., Intel MKL, Python)
â”‚   â””â”€â”€ req_paths.py                  # Helper to set DLL & module paths (*Important*)
â”‚
â”œâ”€â”€ retrieval_data/                   # Serialized precomputed data (decompressed from retrieval_data.tar.gz)
â”‚   â”œâ”€â”€ article_map.json              # Maps article IDs to text or categories
â”‚   â”œâ”€â”€ bm25_*.npy / *.json           # Token frequencies, lengths, IDF values for BM25
â”‚   â”œâ”€â”€ faiss_*.npy / .faiss          # Dense embeddings and FAISS index
â”‚   â””â”€â”€ tokenized_corpus.json         # Tokenized version of the full corpus
â”‚
â”œâ”€â”€ retrieval_lib/                    # Python library code
â”‚   â”œâ”€â”€ __init__.py                   # Library init file
â”‚   â”œâ”€â”€ retrieval.py                  # Retrieval logic (BM25, FAISS integration)
â”‚   â””â”€â”€ tokenizer.py                  # Custom tokenizer subclassing HuggingFace
â”‚
â”œâ”€â”€ trash/                            # Deprecated or unused code (temporary)
â”‚
â”œâ”€â”€ data_preprocess.ipynb             # Preprocessing pipeline (creating retrieval data)
â”œâ”€â”€ dataset_aquisition.ipynb          # Script for downloading/curating data (Getting Test Data)
â””â”€â”€ llama.ipynb                       # Inference notebook using Llama 3.2 1B CPU (main)
