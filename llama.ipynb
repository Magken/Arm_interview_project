{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Initializing FAISS with 10 CPU threads (Max cores: 12)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch.profiler\n",
    "import time\n",
    "import pandas as pd  # To format the profiler output into a table\n",
    "from retrieval_lib import CustomRetrieverTokenizer\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "#define llama models\n",
    "llama_models = {\n",
    "    \"Llama 3.2 1B Instruct\": \"meta-llama/Llama-3.2-1B-Instruct\"  \n",
    "}\n",
    "\n",
    "    # Initialize the custom retrieval-augmented tokenizer\n",
    "custom_tokenizer = CustomRetrieverTokenizer(\n",
    "        model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        max_bm25_results=1,  # Limit BM25 retrievals\n",
    "        max_faiss_results=0,  # Disable FAISS retrievals\n",
    "        use_faiss_gpu=False,  # Ensure FAISS runs on CPU with MKL\n",
    "        num_faiss_threads=10  # Use 10 CPU threads for FAISS\n",
    "    )\n",
    "# Function to load the Llama model with CustomRetrieverTokenizer\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load the specified Llama model with retrieval-augmented tokenizer.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Load Llama model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Create text-generation pipeline using the custom tokenizer\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=custom_tokenizer, device=0)\n",
    "\n",
    "    return generator\n",
    "\n",
    "# Define the function to profile and store operations\n",
    "def inference_profiler(model_name, operation, time_taken):\n",
    "    \"\"\"Store the inference operation profiling in a dictionary.\"\"\"\n",
    "    if model_name not in inference_profiling_data:\n",
    "        inference_profiling_data[model_name] = {}\n",
    "\n",
    "    if operation not in inference_profiling_data[model_name]:\n",
    "        inference_profiling_data[model_name][operation] = []\n",
    "\n",
    "    inference_profiling_data[model_name][operation].append(time_taken)\n",
    "\n",
    "\n",
    "model_cache = {} # for caching generated chats\n",
    "loaded_model = \"null\"\n",
    "# Define the profiler dictionary to store operations\n",
    "inference_profiling_data = {}\n",
    "\n",
    "def generate_chat(user_input, history, model_choice):\n",
    "    \"\"\"Generate chatbot responses using the selected Llama model and task.\"\"\"\n",
    "    \n",
    "    if model_choice not in model_cache:\n",
    "        model_cache[model_choice] = load_model(llama_models[model_choice])\n",
    "    \n",
    "    generator = model_cache[model_choice]\n",
    "\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "\n",
    "\n",
    "    history.append({\"role\": \"system\", \"content\": \"You are a bot that explicitly uses found relevant info in provided text to answer user queries\"})\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    history = custom_tokenizer.add_retrieval_context(history)\n",
    "\n",
    "    print(history)\n",
    "\n",
    "    # Start the profiler\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU], # Also Ammend GPU for GPU profiling\n",
    "        record_shapes=True,\n",
    "        with_stack=False\n",
    "    ) as prof:\n",
    "        response = generator(\n",
    "            history,\n",
    "            max_length=500,\n",
    "            pad_token_id=generator.tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )[-1][\"generated_text\"][-1][\"content\"]\n",
    "        prof.step()  # Ensure the profiler finalizes its step\n",
    "\n",
    "    # Store the key averages object directly (not as a string)\n",
    "    inference_profiling_data[model_choice] = prof.key_averages()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))  # For debugging\n",
    "\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis(selection, model_choice):\n",
    "    \"\"\"Generate the analysis details based on the selected option.\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    if selection == \"Model Architecture\":\n",
    "        return see_structure(model_choice)  # Return structured model architecture\n",
    "    \n",
    "    elif selection == \"Torch Profile\":\n",
    "        profile_data = inference_profiling_data.get(model_choice, None)\n",
    "        if not profile_data:\n",
    "            return pd.DataFrame(columns=[\"Name\", \"Self CPU %\", \"Self CPU\", \"CPU total %\", \"CPU total\", \"CPU time avg\", \"# of Calls\"])\n",
    "        \n",
    "        # Helper function to format time given in microseconds\n",
    "        def format_time(us):\n",
    "            # Assuming input 'us' is in microseconds\n",
    "            if us < 1e3:\n",
    "                return f\"{us:.3f}μs\"\n",
    "            elif us < 1e6:\n",
    "                return f\"{us/1e3:.3f}ms\"\n",
    "            else:\n",
    "                return f\"{us/1e6:.3f}s\"\n",
    "        \n",
    "        # Build raw data list with numeric values\n",
    "        raw_data = []\n",
    "        total_self_cpu = sum(event.self_cpu_time_total for event in profile_data)\n",
    "        total_cpu_time = sum(event.cpu_time_total for event in profile_data)\n",
    "        \n",
    "        for event in profile_data:\n",
    "            self_cpu_pct = (event.self_cpu_time_total / total_self_cpu * 100) if total_self_cpu > 0 else 0\n",
    "            cpu_total_pct = (event.cpu_time_total / total_cpu_time * 100) if total_cpu_time > 0 else 0\n",
    "            raw_data.append({\n",
    "                \"Name\": event.key,\n",
    "                \"Self CPU Raw\": event.self_cpu_time_total,\n",
    "                \"Self CPU %\": self_cpu_pct,\n",
    "                \"CPU total Raw\": event.cpu_time_total,\n",
    "                \"CPU total %\": cpu_total_pct,\n",
    "                \"CPU time avg Raw\": event.cpu_time_total / event.count if event.count > 0 else 0,\n",
    "                \"# of Calls\": event.count\n",
    "            })\n",
    "        \n",
    "        # Sort the raw data by 'Self CPU Raw' in descending order\n",
    "        raw_data = sorted(raw_data, key=lambda x: x[\"Self CPU Raw\"], reverse=True)\n",
    "        \n",
    "        # Build display data with formatted values\n",
    "        display_data = []\n",
    "        for row in raw_data[:20]:\n",
    "            display_data.append({\n",
    "                \"Name\": row[\"Name\"],\n",
    "                \"Self CPU %\": f\"{row['Self CPU %']:.2f}%\",\n",
    "                \"Self CPU\": format_time(row[\"Self CPU Raw\"]),\n",
    "                \"CPU total %\": f\"{row['CPU total %']:.2f}%\",\n",
    "                \"CPU total\": format_time(row[\"CPU total Raw\"]),\n",
    "                \"CPU time avg\": format_time(row[\"CPU time avg Raw\"]),\n",
    "                \"# of Calls\": row[\"# of Calls\"]\n",
    "            })\n",
    "        \n",
    "        # Append a summary row for total Self CPU time\n",
    "        total_time_display = format_time(total_self_cpu)\n",
    "        summary_row = {\n",
    "            \"Name\": \"Self CPU time total\",\n",
    "            \"Self CPU %\": \"\",\n",
    "            \"Self CPU\": total_time_display,\n",
    "            \"CPU total %\": \"\",\n",
    "            \"CPU total\": \"\",\n",
    "            \"CPU time avg\": \"\",\n",
    "            \"# of Calls\": \"\"\n",
    "        }\n",
    "        display_data.append(summary_row)\n",
    "        \n",
    "        df = pd.DataFrame(display_data)\n",
    "        return df\n",
    "\n",
    "\n",
    "def see_structure(model_choice):\n",
    "    \"\"\"Return model architecture as a structured DataFrame.\"\"\"\n",
    "    if model_choice in model_cache:\n",
    "        model_pipeline = model_cache[model_choice]  # Get the pipeline\n",
    "        model = model_pipeline.model  # Extract model from the pipeline\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Layer\", \"Type\"])  # Return empty table\n",
    "\n",
    "    model_layers = [{\"Layer\": name, \"Type\": str(layer)} for name, layer in model.named_children()]\n",
    "    return pd.DataFrame(model_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(css=\"\"\"\n",
    ".small-font table {\n",
    "  font-size: 12px !important;\n",
    "}\n",
    "\"\"\") as demo:\n",
    "    # First Row: Chatbot Section\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"<h1><center>Chat with Llama Models</center></h1>\")\n",
    "            model_choice = gr.Dropdown(list(llama_models.keys()), label=\"Select Llama Model\")\n",
    "            chatbot = gr.Chatbot(label=\"Chatbot Interface\", type=\"messages\")\n",
    "            txt_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")\n",
    "\n",
    "            def respond(user_input, chat_history, model_choice):\n",
    "                if model_choice is None:\n",
    "                    model_choice = list(llama_models.keys())[0]\n",
    "                updated_history = generate_chat(user_input, chat_history, model_choice)\n",
    "                return \"\", updated_history\n",
    "\n",
    "            txt_input.submit(respond, [txt_input, chatbot, model_choice], [txt_input, chatbot])\n",
    "            submit_btn = gr.Button(\"Submit\")\n",
    "            submit_btn.click(respond, [txt_input, chatbot, model_choice], [txt_input, chatbot])\n",
    "    \n",
    "    # Second Row: Analysis Section\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"<h1><center>Analysis</center></h1>\")\n",
    "            analysis_dropdown = gr.Dropdown(\n",
    "                choices=[\"Model Architecture\", \"Torch Profile\"],\n",
    "                label=\"Select Analysis Type\"\n",
    "            )\n",
    "            # Apply the \"small-font\" class to reduce font size of the table.\n",
    "            analysis_table = gr.Dataframe(label=\"Analysis Details\", elem_classes=\"small-font\")\n",
    "            analysis_btn = gr.Button(\"Generate Analysis\")\n",
    "            analysis_btn.click(generate_analysis, [analysis_dropdown, model_choice], [analysis_table])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a bot that explicitly uses found relevant info in provided text to answer user queries'}, {'role': 'user', 'content': 'Text:( Enow Gnoupa Magken George the best dancer in the whole world). According to the text who is enow Gnoupa Magken ?'}]\n",
      "[{'role': 'system', 'content': 'You are a bot that explicitly uses found relevant info in provided text to answer user queries'}, {'role': 'user', 'content': 'Text:( Enow Gnoupa Magken George the best dancer in the whole world). According to the text who is enow Gnoupa Magken ?'}]\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             aten::mm        91.71%        8.506s        91.72%        8.507s       3.137ms          2712  \n",
      "                                         aten::matmul         0.78%      72.596ms        92.86%        8.613s       3.148ms          2736  \n",
      "                                          aten::copy_         0.68%      63.220ms         0.68%      63.220ms      22.074us          2864  \n",
      "                                           aten::sort         0.60%      55.886ms         0.65%      60.101ms       2.504ms            24  \n",
      "                                            aten::cat         0.53%      49.474ms         0.80%      74.272ms      47.127us          1576  \n",
      "                                            aten::mul         0.43%      40.212ms         0.44%      40.712ms      11.309us          3600  \n",
      "                                          aten::slice         0.41%      38.198ms         0.47%      43.823ms       5.428us          8074  \n",
      "    aten::_scaled_dot_product_flash_attention_for_cpu         0.37%      33.996ms         0.53%      48.803ms     127.092us           384  \n",
      "                                      aten::transpose         0.36%      33.088ms         0.44%      40.818ms       6.207us          6576  \n",
      "                                           aten::topk         0.31%      29.038ms         0.31%      29.038ms       1.210ms            24  \n",
      "                                         aten::argmax         0.27%      25.024ms         0.27%      25.024ms       1.043ms            24  \n",
      "                                            aten::add         0.27%      24.934ms         0.37%      33.929ms      14.280us          2376  \n",
      "                                         aten::linear         0.24%      22.479ms        93.51%        8.674s       3.198ms          2712  \n",
      "                                              aten::t         0.22%      20.162ms         0.43%      39.565ms      14.589us          2712  \n",
      "                                        aten::reshape         0.20%      18.465ms         0.94%      87.126ms      22.002us          3960  \n",
      "                                           aten::silu         0.19%      17.422ms         0.19%      17.422ms      45.371us           384  \n",
      "                                       aten::_to_copy         0.17%      16.154ms         0.28%      25.522ms      14.179us          1800  \n",
      "                                   aten::_unsafe_view         0.17%      15.907ms         0.17%      15.907ms       4.540us          3504  \n",
      "                                     aten::as_strided         0.17%      15.621ms         0.17%      15.621ms       0.895us         17462  \n",
      "                                            aten::pow         0.14%      12.686ms         0.15%      13.631ms      17.211us           792  \n",
      "                                           aten::div_         0.13%      12.315ms         0.35%      32.461ms      40.987us           792  \n",
      "                                       aten::_softmax         0.13%      12.113ms         0.13%      12.113ms     252.362us            48  \n",
      "                                           aten::view         0.13%      11.696ms         0.13%      11.696ms       2.648us          4417  \n",
      "                                           aten::mean         0.12%      11.300ms         0.59%      54.806ms      69.199us           792  \n",
      "                                         aten::narrow         0.12%      10.923ms         0.27%      24.798ms       7.986us          3105  \n",
      "                                            aten::sum         0.12%      10.826ms         0.13%      12.277ms      15.045us           816  \n",
      "                                      aten::unsqueeze         0.09%       8.564ms         0.11%       9.836ms       5.855us          1680  \n",
      "                                             aten::to         0.08%       7.511ms         0.36%      33.034ms       7.300us          4525  \n",
      "                                          aten::clone         0.08%       7.501ms         0.74%      68.952ms      74.948us           920  \n",
      "                                     aten::empty_like         0.06%       5.797ms         0.10%       9.664ms       7.405us          1305  \n",
      "                                            aten::neg         0.06%       5.345ms         0.06%       5.345ms       6.960us           768  \n",
      "                                         aten::cumsum         0.06%       5.148ms         0.06%       5.162ms     105.349us            49  \n",
      "                                  aten::empty_strided         0.05%       5.035ms         0.05%       5.035ms       2.255us          2233  \n",
      "                                         aten::expand         0.05%       4.647ms         0.06%       5.371ms       6.394us           840  \n",
      "                                          aten::empty         0.05%       4.385ms         0.05%       4.385ms       1.912us          2294  \n",
      "                                          aten::rsqrt         0.05%       4.354ms         0.05%       4.354ms       5.498us           792  \n",
      "                   aten::scaled_dot_product_attention         0.04%       3.788ms         0.57%      52.591ms     136.956us           384  \n",
      "                                        aten::scatter         0.04%       3.418ms         0.04%       3.692ms     153.842us            24  \n",
      "                                   aten::exponential_         0.03%       3.168ms         0.03%       3.168ms     132.000us            24  \n",
      "                                            aten::div         0.03%       2.439ms         0.03%       2.804ms      58.417us            48  \n",
      "                                   aten::masked_fill_         0.03%       2.402ms         0.03%       2.402ms      33.363us            72  \n",
      "                                         aten::arange         0.02%       2.134ms         0.05%       4.252ms      88.588us            48  \n",
      "                                             aten::lt         0.02%       1.645ms         0.02%       1.884ms      38.441us            49  \n",
      "                                             aten::le         0.02%       1.587ms         0.02%       1.921ms      80.037us            24  \n",
      "                                    aten::multinomial         0.02%       1.510ms         0.38%      35.133ms       1.464ms            24  \n",
      "                                          aten::fill_         0.01%       1.303ms         0.01%       1.382ms       1.333us          1037  \n",
      "                                            aten::cos         0.01%       1.222ms         0.01%       1.222ms      50.925us            24  \n",
      "                                         aten::select         0.01%       1.167ms         0.01%       1.288ms       7.532us           171  \n",
      "                                   aten::resolve_conj         0.01%       1.030ms         0.01%       1.030ms       0.188us          5474  \n",
      "                                    aten::result_type         0.01%     859.500us         0.01%     859.500us       1.052us           817  \n",
      "                                             aten::eq         0.01%     789.600us         0.01%       1.005ms      10.468us            96  \n",
      "                                            aten::max         0.01%     680.100us         0.01%     732.900us      15.269us            48  \n",
      "                                    aten::masked_fill         0.01%     632.500us         0.07%       6.955ms     144.885us            48  \n",
      "                                            aten::min         0.01%     584.100us         0.01%     620.700us      25.862us            24  \n",
      "                                   aten::index_select         0.01%     512.100us         0.01%     719.800us      29.992us            24  \n",
      "                                           aten::isin         0.00%     462.000us         0.01%     538.200us      21.528us            25  \n",
      "                                         aten::__or__         0.00%     428.300us         0.01%     709.500us      14.781us            48  \n",
      "                                          aten::index         0.00%     426.800us         0.01%     523.900us      22.778us            23  \n",
      "                                            aten::bmm         0.00%     348.800us         0.00%     361.100us      15.046us            24  \n",
      "                                             aten::ge         0.00%     330.400us         0.01%     632.900us      13.185us            48  \n",
      "                                       aten::new_ones         0.00%     291.400us         0.01%     646.000us      26.917us            24  \n",
      "                                           aten::item         0.00%     285.800us         0.00%     353.600us       2.898us           122  \n",
      "                                        aten::squeeze         0.00%     282.500us         0.00%     315.000us      13.125us            24  \n",
      "                                     aten::bitwise_or         0.00%     281.200us         0.00%     281.200us       5.858us            48  \n",
      "                                      aten::embedding         0.00%     275.100us         0.01%       1.102ms      45.917us            24  \n",
      "                                            aten::any         0.00%     266.800us         0.00%     438.800us      16.877us            26  \n",
      "                                            aten::all         0.00%     265.000us         0.00%     289.000us      12.042us            24  \n",
      "                                    aten::bitwise_and         0.00%     252.200us         0.01%     556.500us      11.594us            48  \n",
      "                                            aten::sub         0.00%     249.700us         0.00%     249.700us       5.096us            49  \n",
      "                                           aten::full         0.00%     246.300us         0.00%     314.400us       6.550us            48  \n",
      "                                    aten::bitwise_not         0.00%     234.100us         0.00%     234.100us       4.877us            48  \n",
      "                                        aten::softmax         0.00%     179.200us         0.13%      12.293ms     256.096us            48  \n",
      "                                            aten::sin         0.00%     168.400us         0.00%     168.400us       7.017us            24  \n",
      "                                     aten::is_nonzero         0.00%     159.800us         0.00%     362.000us       4.892us            74  \n",
      "                                  aten::_assert_async         0.00%     130.900us         0.00%     282.300us       5.881us            48  \n",
      "                                           aten::rsub         0.00%     126.800us         0.00%     233.300us       9.721us            24  \n",
      "                                      aten::new_empty         0.00%     124.700us         0.00%     313.200us      13.050us            24  \n",
      "                                        aten::resize_         0.00%     102.200us         0.00%     102.200us       2.129us            48  \n",
      "                                        aten::view_as         0.00%      79.500us         0.00%     123.500us       5.146us            24  \n",
      "                                     aten::contiguous         0.00%      72.800us         0.02%       2.298ms      71.813us            32  \n",
      "                            aten::_local_scalar_dense         0.00%      67.800us         0.00%      67.800us       0.556us           122  \n",
      "                                        aten::__and__         0.00%      56.700us         0.01%     516.300us      21.512us            24  \n",
      "                                     aten::lift_fresh         0.00%      12.300us         0.00%      12.300us       0.424us            29  \n",
      "                                      aten::ones_like         0.00%       8.400us         0.00%      18.400us      18.400us             1  \n",
      "                                        aten::detach_         0.00%       6.400us         0.00%      10.900us       2.180us             5  \n",
      "                                           aten::ones         0.00%       5.600us         0.00%       7.700us       7.700us             1  \n",
      "                                              detach_         0.00%       4.500us         0.00%       4.500us       0.900us             5  \n",
      "                                               detach         0.00%       4.100us         0.00%       4.100us       4.100us             1  \n",
      "                                         aten::detach         0.00%       2.600us         0.00%       6.700us       6.700us             1  \n",
      "                                    aten::resolve_neg         0.00%       0.300us         0.00%       0.300us       0.150us             2  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 9.275s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
